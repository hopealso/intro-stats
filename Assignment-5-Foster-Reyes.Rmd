---
title: "Introduction to Statistical Concepts, MSCA 31000, Assignment 5"
author: "Hope Foster-Reyes"
date: "April 2, 2016"
output: html_document
header-includes: \usepackage{amsmath}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(reshape2)
#library(ggplot2)
```

## Chapter 15, Q1.

In an Analysis of Variance the null hypothesis is the hypothesis that all four population means are equal. When rejected, the conclusion is that at least one population mean is different from at least one population mean. Because of this lack of specificity, it is called a "non-specific null hypothesis" or the "omnibus null hypothesis".

## Chapter 15, Q3.

A between-subjects variable is a variable whose value or statistic/parameter is compared between different groups of subjects, in contrast to a within-subject variable whose value is compared for different conditions exposed to the same group of subjects. 

## Chapter 15, Q5.

Within an ANOVA test, the symbol N may be used to denote the the total number of observations, and the symbol n to denote the number of observations in each group.

## Chapter 15, Q7.

The central limit theorem allows us to estimate the population variance as the sample size (n) multiplied by the sampling distribution of the mean. 

When calculating MSB, we are estimating the population variance. 

We can then calculate MSE (our estimate of the population variance) as the sample size (n) multiplied by the the variance of the sample means (our estimate of the sampling distribution of the mean).

## Chapter 15, Q9.

MSE and MSB measure the same quantity, the population variance, when the population means for all groups are the same.

## Chapter 15, Q11.

The value of F in Fisher's F distribution measures the ratio of MSB to MSE. The probability density of the F distribution approaches zero as the ratio of MSB to MSE increases, and its shape is depends on the degrees of freedom of the numerator (MSB) and the denominator (MSE).

## Chapter 15, Q13.

The mean square total could be computed by dividing the sum of squares by the degrees of freedom. 

## Chapter 15, Q15.

### a.

Interaction

### b.

Main effect

### c.

Simple effect

### d.

Specific comparison

### e.

Simple effect

## Chapter 15, Q17.

### Summarize data

```{r 15-q7}
# load data from exercise
rc.data <- data.frame(time=rep(30, 5), 
                      age=rep(12, 5), 
                      rc=c(66, 68, 59, 72, 46))
rc.data <- rbind(rc.data, data.frame(time=rep(30, 5), 
                                     age=rep(16, 5), 
                                     rc=c(74, 71, 67, 82, 76)))
rc.data <- rbind(rc.data, data.frame(time=rep(60, 5), 
                                     age=rep(12, 5), 
                                     rc=c(69, 61, 69, 73, 61)))
rc.data <- rbind(rc.data, data.frame(time=rep(60, 5), 
                                     age=rep(16, 5), 
                                     rc=c(95, 92, 95, 98, 94)))

boxplot(rc~time, data = rc.data, main="Reading Comprehension by Time")
boxplot(rc~age, data = rc.data, main="Reading Comprehension by Age")

rc.groupmean <- aggregate(list(mean=rc.data$rc), by = list(time = rc.data$time, age = rc.data$age), FUN=mean)
rc.groupmean

rc.groupvar <- aggregate(list(var=rc.data$rc), by = list(time = rc.data$time, age = rc.data$age), FUN=var)
rc.groupvar

```



### Null hypothesis



### Test

```{r q17-compute}
rc.size <- nrow(rc.data)
rc.group.size <- rc.size / nrow(rc.groupmean)
rc.grandmean <- mean(rc.groupmean$mean)

#MSE <- mean(rc.groupvar)
#MSB <- rc.group.size * var(rc.groupmean)

SSTot <- sum((rc.data$rc - rc.grandmean)^2)
SSTime <- rc.group.size * length(unique(rc.data$time)) * 
  sum((with(rc.data, tapply(rc, rc.data$time, mean)) - rc.grandmean)^2)
SSAge <- rc.group.size * length(unique(rc.data$age)) * 
  sum((with(rc.data, tapply(rc, rc.data$age, mean)) - rc.grandmean)^2)

sqerr <- function(age, time, mean) {
  return(sum((rc.data$rc[rc.data$age == age & rc.data$time == time] - mean)^2))
}

```

## Chapter 15, Q19.

There is an interaction when simple effects differ.

## Chapter 15, Q21.



## Chapter 15, Q23.

Within-subjetcs designs are typically more powerful than between-subjects design because the natural variations among subjects are controlled. By testing the same individuals in different conditions, we reduce the individual differences that would lead to increased variations in the variable that we are attempting to study.

### Prepare Environment

Load libraries.

```{r libraries}
library(xlsx)
```

Download data from online Url to subdirectory local to this RStudio project.

```{r download-data}
if (!file.exists("data")) {
  dir.create("data")
}

# !!!This version of file via automated download fails. 
# Remainder of code for assigment assumes manually imported version of .xlsx file located 
# in data subdirectory of working (.rproj) directory
am.url <- "http://onlinestatbook.com/2/case_studies/data/angry_moods.xls"
am.file <- "./data/angry_moods.xls"
if (!file.exists(am.file)) {
  download.file(am.url, destfile = am.file)
}
```

Import data.

```{r import-data}
am.df <- read.xlsx(am.file, 
                   sheetName = "angry_moods.txt", 
                   header = TRUE,
                   colClasses = rep("integer", 7))
```

### Observe scatter plot

```{r q18-plot}
plot(am.df$Control.Out, am.df$Anger.Out)
```

### Find regression line for predicting Anger.Out from Control.Out

```{r q18}
coao.n <- nrow(am.df)
co.mean <- mean(am.df$Control.Out)
ao.mean <- mean(am.df$Anger.Out)
cor.coao <- cor(am.df$Control.Out, am.df$Anger.Out, method = "pearson")

# calculate slope and intercept of simple linear regression  
b.ao <- cor.coao * (sd(am.df$Anger.Out) / sd(am.df$Control.Out))
a.ao <- ao.mean - b.ao * co.mean
coao.predicted <- b.ao * am.df$Control.Out + a.ao
coao.residual <- am.df$Anger.Out - coao.predicted

# collect data points into a data frame
coao.regression <- data.frame(seq(1, coao.n), am.df$Control.Out, am.df$Anger.Out, 
                              coao.predicted, coao.residual, coao.residual^2)
colnames(coao.regression) <- c("id", "Control.Out", "Anger.Out", 
                               "AO.Predicted", "AO.Resid", "AO.SqResid")

# Check result using lm()
coao.lm <- lm(Anger.Out ~ Control.Out, data = coao.regression)
coao.coeffs <- coefficients(coao.lm)
```

$\hat{y}_{AO} = bx_{CO} + a$

### a.

$b = r_{COAO} \frac{\sigma_{AO}}{\sigma_{CO}}$

$r_{COAO} = `r cor.coao`$

$b = r_{COAO} \frac{\sigma_{AO}}{\sigma_{CO}} = (`r cor.coao`)\frac{`r sd(am.df$Anger.Out)`}{`r sd(am.df$Control.Out)`} = `r b.ao`$ 

### b.

$a = \bar{y}_{AO} - b\bar{x}_{CO} = `r ao.mean` - (`r b.ao`)`r co.mean` = `r a.ao`$

Regression line:

$\hat{y}_{AO} = `r b.ao`x_{CO} + `r a.ao`$

### c.

Plot regression, calculated manually:

```{r q18-test}
plot(coao.regression$Control.Out, coao.regression$Anger.Out, col = "blue", main = "Manual Linear Regression")
points(coao.regression$Control.Out, coao.regression$AO.Predicted, col = "red")
```

Plot histogram of residuals.

```{r q18-resid}
hist(coao.regression$AO.Resid)
```

The relationship appears to be approximately linear. The errors of prediction appear to be distributed normally. However, further analysis may be necessary to determine if the distribution is homoscedastic as the variance around the regression line does not appear wholly consistent.

### d.

#### Null hypothesis:

$H_{0}: b = 0$

$H_{A}: b <> 0$

#### Test:

```{r q18-signif}
coao.ssx <- sum((am.df$Control.Out - co.mean)^2)
coao.ssy <- sum((am.df$Anger.Out - ao.mean)^2)
coao.see.r <- sqrt(((1 - cor.coao^2) * coao.ssy) / (coao.n - 2))
coao.see <- sqrt(sum(coao.regression$AO.SqResid) / (coao.n - 2))
coao.sd.b <- coao.see / sqrt(coao.ssx)

coao.t <- b.ao / coao.sd.b

coao.p <- 2 * pt(-coao.t, df = coao.n - 2, lower.tail = FALSE)
coao.t.05 <- qt(1 - .05 / 2, df = coao.n - 2)
```

$t = \frac{statistic - \text{hypothesized value}}{\text{standard error of the statistic}}$

Where $b$ is the slope of the regression line predicting Anger.Out from Control.Out, and $s_{b}$ is the standard error of $b$:

$t = \frac{b - 0}{s_{b}}$

Where $s_{est}$ is the standard error of the estimate, SSX is the sum of squared deviations of Control.Out ($x_{CO}$) from $\bar{x}_{CO}$ and SSY is the sum of squared deviations of Anger.Out ($x_{AO}$) from $\bar{y}_{AO}$:

$s_{b} = \frac{s_{est}}{\sqrt{SSX}}$

$SSX = \sum (x_{CO}-\bar{x}_{CO})^2 = `r coao.ssx`$

$s_{est} = \sqrt{\frac{(1-r^2)SSY}{N - 2}}$

$SSY = \sum (y_{AO}-\bar{y}_{AO})^2 = `r coao.ssy`$

$s_{est} = \sqrt{\frac{(1-`r cor.coao`^2)`r coao.ssy`}{`r coao.n` - 2}} = `r coao.see`$

$s_{b} = \frac{s_{est}}{\sqrt{SSX}} = \frac{`r coao.see`}{\sqrt{`r coao.ssx`}} = `r coao.sd.b`$

$t = \frac{b - 0}{s_{b}} = \frac{`r b.ao`}{`r coao.sd.b`} = `r coao.t`$

What is the probability of getting a t as large or larger or as small or smaller in our sampling distribution?

Degrees of Freedom = `r coao.n - 2`

$P(t < `r coao.t` \; or \; t > `r -coao.t`) = `r coao.p`$

Alternately, what are the t critical values for a two-tailed test of alpha = 0.5 and df = `r coao.n - 2`?

$`r -coao.t.05`, `r coao.t.05`$

We reject the null hypothesis; the slope is significantly different from 0 within 95% confidence.

### e.

$s_{est} = \sqrt{\frac{(1-r^2)SSY}{N - 2}}$

$SSY = \sum (y_{AO}-\bar{y}_{AO})^2 = `r coao.ssy`$

$s_{est} = \sqrt{\frac{(1-`r cor.coao`^2)`r coao.ssy`}{`r coao.n` - 2}} = `r coao.see`$

## Chapter 16, Q1.

Log transformations can be used to make highly skewed distributions less skewed. This can be valuable both for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics.

## Chapter 16, Q2.

$10^3 = 1000$

## Chapter 16, Q3.

$9^{\lambda} = 9^.5 = `r sqrt(9)`$

$16^{\lambda} = 16^.5 = `r sqrt(16)`$

$25^{\lambda} = 25^.5 = `r sqrt(25)`$

Transformed data using $\lambda$ = .5: (3, 4, 5)

## Chapter 16, Q4.

$\lambda$ = 0 in Tukey's ladder decreases skew the most.

## Chapter 16, Q5.

For positively skewed data, higher values of $\lambda$ increase skew the most, the value of *2* in Tukey's ladder being the highest.

For negatively skewed data, lower values of $\lambda$ increase skew the most, the value of *-2* in Tukey's ladder being the lowest.
