---
title: "Introduction to Statistical Concepts, MSCA 31000, Assignment 5"
author: "Hope Foster-Reyes"
date: "April 2, 2016"
output: pdf_document
header-includes: \usepackage{amsmath}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(reshape2)
#library(ggplot2)
```

## Chapter 15, Q1.

In an Analysis of Variance the null hypothesis is the hypothesis that all four population means are equal. When rejected, the conclusion is that at least one population mean is different from at least one population mean. Because of this lack of specificity, it is called a "non-specific null hypothesis" or the "omnibus null hypothesis".

## Chapter 15, Q3.

A between-subjects variable is a variable whose value or statistic/parameter is compared between different groups of subjects, in contrast to a within-subject variable whose value is compared for different conditions exposed to the same group of subjects. 

## Chapter 15, Q5.

Within an ANOVA test, the symbol N may be used to denote the the total number of observations, and the symbol n to denote the number of observations in each group.

## Chapter 15, Q7.

## Chapter 15, Q9.

## Chapter 15, Q11.

## Chapter 15, Q13.

## Chapter 15, Q15.

## Chapter 15, Q17.

## Chapter 14, Q18.

### Prepare Environment

Load libraries.

```{r libraries}
library(xlsx)
```

Download data from online Url to subdirectory local to this RStudio project.

```{r download-data}
if (!file.exists("data")) {
  dir.create("data")
}

# !!!This version of file via automated download fails. 
# Remainder of code for assigment assumes manually imported version of .xlsx file located 
# in data subdirectory of working (.rproj) directory
am.url <- "http://onlinestatbook.com/2/case_studies/data/angry_moods.xls"
am.file <- "./data/angry_moods.xls"
if (!file.exists(am.file)) {
  download.file(am.url, destfile = am.file)
}
```

Import data.

```{r import-data}
am.df <- read.xlsx(am.file, 
                   sheetName = "angry_moods.txt", 
                   header = TRUE,
                   colClasses = rep("integer", 7))
```

### Observe scatter plot

```{r q18-plot}
plot(am.df$Control.Out, am.df$Anger.Out)
```

### Find regression line for predicting Anger.Out from Control.Out

```{r q18}
coao.n <- nrow(am.df)
co.mean <- mean(am.df$Control.Out)
ao.mean <- mean(am.df$Anger.Out)
cor.coao <- cor(am.df$Control.Out, am.df$Anger.Out, method = "pearson")

# calculate slope and intercept of simple linear regression  
b.ao <- cor.coao * (sd(am.df$Anger.Out) / sd(am.df$Control.Out))
a.ao <- ao.mean - b.ao * co.mean
coao.predicted <- b.ao * am.df$Control.Out + a.ao
coao.residual <- am.df$Anger.Out - coao.predicted

# collect data points into a data frame
coao.regression <- data.frame(seq(1, coao.n), am.df$Control.Out, am.df$Anger.Out, 
                              coao.predicted, coao.residual, coao.residual^2)
colnames(coao.regression) <- c("id", "Control.Out", "Anger.Out", 
                               "AO.Predicted", "AO.Resid", "AO.SqResid")

# Check result using lm()
coao.lm <- lm(Anger.Out ~ Control.Out, data = coao.regression)
coao.coeffs <- coefficients(coao.lm)
```

$\hat{y}_{AO} = bx_{CO} + a$

### a.

$b = r_{COAO} \frac{\sigma_{AO}}{\sigma_{CO}}$

$r_{COAO} = `r cor.coao`$

$b = r_{COAO} \frac{\sigma_{AO}}{\sigma_{CO}} = (`r cor.coao`)\frac{`r sd(am.df$Anger.Out)`}{`r sd(am.df$Control.Out)`} = `r b.ao`$ 

### b.

$a = \bar{y}_{AO} - b\bar{x}_{CO} = `r ao.mean` - (`r b.ao`)`r co.mean` = `r a.ao`$

Regression line:

$\hat{y}_{AO} = `r b.ao`x_{CO} + `r a.ao`$

### c.

Plot regression, calculated manually:

```{r q18-test}
plot(coao.regression$Control.Out, coao.regression$Anger.Out, col = "blue", main = "Manual Linear Regression")
points(coao.regression$Control.Out, coao.regression$AO.Predicted, col = "red")
```

Plot histogram of residuals.

```{r q18-resid}
hist(coao.regression$AO.Resid)
```

The relationship appears to be approximately linear. The errors of prediction appear to be distributed normally. However, further analysis may be necessary to determine if the distribution is homoscedastic as the variance around the regression line does not appear wholly consistent.

### d.

#### Null hypothesis:

$H_{0}: b = 0$

$H_{A}: b <> 0$

#### Test:

```{r q18-signif}
coao.ssx <- sum((am.df$Control.Out - co.mean)^2)
coao.ssy <- sum((am.df$Anger.Out - ao.mean)^2)
coao.see.r <- sqrt(((1 - cor.coao^2) * coao.ssy) / (coao.n - 2))
coao.see <- sqrt(sum(coao.regression$AO.SqResid) / (coao.n - 2))
coao.sd.b <- coao.see / sqrt(coao.ssx)

coao.t <- b.ao / coao.sd.b

coao.p <- 2 * pt(-coao.t, df = coao.n - 2, lower.tail = FALSE)
coao.t.05 <- qt(1 - .05 / 2, df = coao.n - 2)
```

$t = \frac{statistic - \text{hypothesized value}}{\text{standard error of the statistic}}$

Where $b$ is the slope of the regression line predicting Anger.Out from Control.Out, and $s_{b}$ is the standard error of $b$:

$t = \frac{b - 0}{s_{b}}$

Where $s_{est}$ is the standard error of the estimate, SSX is the sum of squared deviations of Control.Out ($x_{CO}$) from $\bar{x}_{CO}$ and SSY is the sum of squared deviations of Anger.Out ($x_{AO}$) from $\bar{y}_{AO}$:

$s_{b} = \frac{s_{est}}{\sqrt{SSX}}$

$SSX = \sum (x_{CO}-\bar{x}_{CO})^2 = `r coao.ssx`$

$s_{est} = \sqrt{\frac{(1-r^2)SSY}{N - 2}}$

$SSY = \sum (y_{AO}-\bar{y}_{AO})^2 = `r coao.ssy`$

$s_{est} = \sqrt{\frac{(1-`r cor.coao`^2)`r coao.ssy`}{`r coao.n` - 2}} = `r coao.see`$

$s_{b} = \frac{s_{est}}{\sqrt{SSX}} = \frac{`r coao.see`}{\sqrt{`r coao.ssx`}} = `r coao.sd.b`$

$t = \frac{b - 0}{s_{b}} = \frac{`r b.ao`}{`r coao.sd.b`} = `r coao.t`$

What is the probability of getting a t as large or larger or as small or smaller in our sampling distribution?

Degrees of Freedom = `r coao.n - 2`

$P(t < `r coao.t` \; or \; t > `r -coao.t`) = `r coao.p`$

Alternately, what are the t critical values for a two-tailed test of alpha = 0.5 and df = `r coao.n - 2`?

$`r -coao.t.05`, `r coao.t.05`$

We reject the null hypothesis; the slope is significantly different from 0 within 95% confidence.

### e.

$s_{est} = \sqrt{\frac{(1-r^2)SSY}{N - 2}}$

$SSY = \sum (y_{AO}-\bar{y}_{AO})^2 = `r coao.ssy`$

$s_{est} = \sqrt{\frac{(1-`r cor.coao`^2)`r coao.ssy`}{`r coao.n` - 2}} = `r coao.see`$

## Chapter 16, Q1.

Log transformations can be used to make highly skewed distributions less skewed. This can be valuable both for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics.

## Chapter 16, Q2.

$10^3 = 1000$

## Chapter 16, Q3.

$9^{\lambda} = 9^.5 = `r sqrt(9)`$

$16^{\lambda} = 16^.5 = `r sqrt(16)`$

$25^{\lambda} = 25^.5 = `r sqrt(25)`$

Transformed data using $\lambda$ = .5: (3, 4, 5)

## Chapter 16, Q4.

$\lambda$ = 0 in Tukey's ladder decreases skew the most.

## Chapter 16, Q5.

For positively skewed data, higher values of $\lambda$ increase skew the most, the value of *2* in Tukey's ladder being the highest.

For negatively skewed data, lower values of $\lambda$ increase skew the most, the value of *-2* in Tukey's ladder being the lowest.
