---
title: "Introduction to Statistical Concepts, MSCA 31000, Assignment 4"
author: "Hope Foster-Reyes"
date: "March 26, 2016"
output: html_document
header-includes: \usepackage{amsmath}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#library(reshape2)
#library(ggplot2)
```

## Chapter 14, Q2.

```{r q2}
y.predicted <- function (x, b, a) {
  return((b * x) + a)
}

x.predictor <- function (y, b, a) {
  return((y - a) / b)
}

q2.a <- y.predicted(6, 2, 9)

q2.b <- x.predictor(14, 2, 9)
```

$\hat{y} = bx + a$

$\hat{y} = 2x + 9$

### a.

$x = 6$

$\hat{y} = 2x + 9$

$\hat{y} = (2)6 + 9 = `r q2.a`$

### b.

$\hat{y} = 14$

$\hat{y} = 2x + 9$

$2x = \hat{y} - 9$

$x = (14-9)/2 = `r q2.b`$

## Chapter 14, Q4.

The standard error of the estimate measures the accuracy of predictions of a linear regression. It is the standard deviation of the errors of prediction.

If:

$x$ = criterion variable

$y$ = predictor variable (actual)

$\hat{y}$ = predicted value

$N$ = size of the population

Then the standard error of the estimate (SEE) for a population can be calculated as follows:

SEE (population) = $\sqrt{\frac{\sum {(y - \hat{y})}^2}{N}}$

## Chapter 14, Q6.

Summarize data:

```{r q6}
# test data from book
#q6.x <- c(1,2,3,4,5)
#q6.y <- c(1,2,1.3,3.75,2.25)

# data for exercise
q6.x <- c(2,4,4,5,6)
q6.y <- c(5,6,7,11,12)

q6.x.mean <- mean(q6.x)
q6.y.mean <- mean(q6.y)
x.deviation <- q6.x - q6.x.mean
y.deviation <- q6.y - q6.y.mean
q6.n <- length(q6.x)

cor.xy <- cor(q6.x, q6.y, method = "pearson")
cor.xy.manual <- sum(x.deviation * y.deviation) / 
  sqrt(sum(x.deviation^2) * sum(y.deviation^2))

# calculate slope and intercept of simple linear regression  
b.y <- cor.xy * (sd(q6.y) / sd(q6.x))
a.y <- mean(q6.y) - b.y * mean(q6.x)
q6.predicted <- b.y * q6.x + a.y
q6.residual <- q6.y - q6.predicted

# collect data points into a data frame
q6.data <- data.frame(seq(1, q6.n), q6.x, q6.y, x.deviation, x.deviation^2, y.deviation, y.deviation^2, 
                      q6.predicted, q6.residual, q6.residual^2)
colnames(q6.data) <- c("id", "x", "y", "xdev", "xsqdev", "ydev", "ysqdev", 
                       "ypredicted", "yresid", "ysqresid")

# display
q6.data
round(colSums(q6.data), 3)
```

### a.

#### Null hypothesis:

$H_{0}: r = 0$

$H_{A}: r <> 0$

#### Calculate statistic:

$r_{xy} = \frac{\sum (x - \bar{x})(y - \bar{y})}{\sqrt{\sum (x - \bar{x})^2(y - \bar{y})^2}}$

$r_{xy} = `r cor.xy`$

#### Test hypothesis:

```{r q6-cor}
q6.cor.t <- (cor.xy * sqrt(q6.n - 2)) / sqrt(1 - cor.xy^2)

q6.cor.p <- 2 * pt(q6.cor.t, df = q6.n - 2, lower.tail = FALSE)
q6.t.05 <- qt(1 - .05 / 2, df = q6.n - 2)
```

$t = \frac{r \sqrt{n - 2}}{\sqrt{1 - r^2}}$

$t = \frac{`r cor.xy` \sqrt{`r q6.n` - 2}}{\sqrt{1 - `r cor.xy`^2}} = `r q6.cor.t`$

What is the probability of getting a t as large or larger or as small or smaller in our sampling distribution?

Degrees of Freedom = `r q6.n - 2`

$P(t < -`r q6.cor.t` \; or \; t > `r q6.cor.t`) = `r q6.cor.p`$

Alternately, what are the t critical values for a two-tailed test of alpha = 0.5 and df = `r q6.n - 2`?

$`r -q6.t.05`, `r q6.t.05`$

We reject the null hypothesis; the correlation is significantly different from 0 within 95% confidence.

### b.

#### Null hypothesis:

$H_{0}: b = 0$

$H_{A}: b <> 0$

#### Calculate statistic:

$b = r_{xy} \frac{\sigma_{y}}{\sigma_{x}}$

$r_{xy} = `r cor.xy`$

$b = r_{xy} \frac{\sigma_{y}}{\sigma_{x}} = (`r cor.xy`)\frac{`r sd(q6.y)`}{`r sd(q6.x)`} = `r b.y`$ 

#### Test hypothesis:

```{r q6-b}
q6.ssx <- sum(x.deviation^2)
q6.ssy <- sum(y.deviation^2)
q6.see.r <- sqrt(((1 - cor.xy^2) * q6.ssy) / (q6.n - 2))
q6.see <- sqrt(sum(q6.data$ysqresid) / (q6.n - 2))
q6.sd.b <- q6.see / sqrt(q6.ssx)

q6.t <- b.y / q6.sd.b

q6.p <- 2 * pt(q6.t, df = q6.n - 2, lower.tail = FALSE)
```

$t = \frac{statistic - \text{hypothesized value}}{\text{standard error of the statistic}}$

Where $b$ is the slope of the regression line predicting $y$ from $x$, and $s_{b}$ is the standard error of $b$:

$t = \frac{b - 0}{s_{b}}$

Where $s_{est}$ is the standard error of the estimate, SSX is the sum of squared deviations of $x$ from $\bar{x}$ and SSY is the sum of squared deviations of $y$ from $\bar{y}$:

$s_{b} = \frac{s_{est}}{\sqrt{SSX}}$

$SSX = \sum (x-\bar{x})^2 = `r q6.ssx`$

$s_{est} = \sqrt{\frac{(1-r^2)SSY}{N - 2}}$

$SSY = \sum (y-\bar{y})^2 = `r q6.ssy`$

$s_{est} = \sqrt{\frac{(1-`r cor.xy`^2)`r q6.ssy`}{`r q6.n` - 2}} = `r q6.see`$

$s_{b} = \frac{s_{est}}{\sqrt{SSX}} = \frac{`r q6.see`}{\sqrt{`r q6.ssx`}} = `r q6.sd.b`$

$t = \frac{b - 0}{s_{b}} = \frac{`r b.y`}{`r q6.sd.b`} = `r q6.t`$

What is the probability of getting a t as large or larger or as small or smaller in our sampling distribution?

Degrees of Freedom = `r q6.n - 2`

$P(t < -`r q6.t` \; or \; t > `r q6.t`) = `r q6.p`$

Alternately, what are the t critical values for a two-tailed test of alpha = 0.5 and df = `r q6.n - 2`?

$`r -q6.t.05`, `r q6.t.05`$

We reject the null hypothesis; the slope is significantly different from 0 within 95% confidence.

### c.

#### Find t critical value:

```{r q6-ci}
Y.95 <- 0.95
t.95 <- qt((1 - Y.95)/2, df = q6.n - 2, lower.tail = FALSE)

q6.LL <- b.y - (t.95 * q6.sd.b)
q6.UL <- b.y + (t.95 * q6.sd.b)
```

Degrees of Freedom = `r q6.n - 2`

95% t Critical Value= `r t.95`

#### Calculate confidence interval:

Lower Limit = $b - (t_{.95})(s_{b}) = `r b.y` - (`r t.95`)(`r q6.sd.b`) = `r q6.LL`$

Upper Limit = $b + (t_{.95})(s_{b}) = `r b.y` + (`r t.95`)(`r q6.sd.b`) = `r q6.UL`$

## Chapter 14, Q8.

$r = 0.4$

#### Null hypothesis:

$H_{0}: r = 0$

$H_{A}: r <> 0$

#### Test hypothesis:

```{r q8-cor}
q8.cor.t <- (0.4 * sqrt(20 - 2)) / sqrt(1 - 0.4^2)

q8.cor.p <- 2 * pt(q8.cor.t, df = 20 - 2, lower.tail = FALSE)
q8.t.05 <- qt(1 - .05 / 2, df = 20 - 2)
```

$t = \frac{r \sqrt{n - 2}}{\sqrt{1 - r^2}}$

$t = \frac{0.4 \sqrt{20 - 2}}{\sqrt{1 - 0.4^2}} = `r q8.cor.t`$

What is the probability of getting a t as large or larger or as small or smaller in our sampling distribution?

Degrees of Freedom = 18

$P(t < -`r q8.cor.t` \; or \; t > `r q8.cor.t`) = `r q8.cor.p`$

Alternately, what are the t critical values for a two-tailed test of alpha = 0.5 and df = 18?

$`r -q8.t.05`, `r q8.t.05`$

The correlation is not significantly different from 0. We fail to reject the null hypothesis.

## Chapter 14, Q10.

```{r q10}
x.pre <- c(59,52,44,51,42,42,41,45,27,63,54,44,50,47,55,49,45,57,46,60,65,64,50,74,59)
y.post <- c(56,63,55,50,66,48,58,36,13,50,81,56,64,50,63,57,73,63,46,60,47,73,58,85,44)
pre.mean <- mean(x.pre)
post.mean <- mean(y.post)
pre.deviation <- x.pre - pre.mean
post.deviation <- y.post - post.mean

cor.prepost <- cor(x.pre, y.post, method = "pearson")
cor.prepost.manual <- sum(pre.deviation * post.deviation) / 
  sqrt(sum(pre.deviation^2) * sum(post.deviation^2))
  
b.post <- cor.prepost * (sd(y.post) / sd(x.pre))
a.post <- mean(y.post) - b.post * mean(x.pre)

post.regression <- data.frame(seq(1, length(x.pre)), x.pre, y.post, 
                              b.post * x.pre + a.post)
colnames(post.regression) <- c("id", "pre", "post", "predicted")

q10 <- y.predicted(43, a = a.post, b = b.post)
```

$\hat{y} = bx + a$

$x = 43$

$b = r_{xy} \frac{\sigma_{y}}{\sigma_{x}}$

$r_{xy} = \frac{\sum (x - \bar{x})(y - \bar{y})}{\sqrt{\sum (x - \bar{x})^2(y - \bar{y})^2}}$

$r_{xy} = `r cor.prepost`$

$b = r_{xy} \frac{\sigma_{y}}{\sigma_{x}} = (`r cor.prepost`)\frac{`r sd(y.post)`}{`r sd(x.pre)`} = `r b.post`$ 

$a = \bar{y} - b\bar{x} = `r post.mean` - (`r b.post`)`r pre.mean` = `r a.post`$

$\hat{y} = (`r b.post`)43 + `r a.post`$

$\hat{y} = `r q10`$

Plot regression, calculated manually:

```{r q10-test}
plot(post.regression$pre, post.regression$post, col = "blue", main = "Manual Linear Regression")
points(post.regression$pre, post.regression$predicted, col = "red")

# Check result using lm()
post.lm <- lm(post ~ pre, data = post.regression)
post.coeffs <- coefficients(post.lm)
q10.test <- y.predicted(43, post.coeffs[2], post.coeffs[1])
```

## Chapter 14, Q12.

$\bar{x} = 10$

$\bar{y} = 12$

$\sigma_{x} = 2.5$

$\sigma_{y} = 3.0$

$r_{xy} = -0.6$

```{r q12}
x.mean <- 10
y.mean <- 12
x.sd <- 2.5
y.sd <- 3.0
r.xy <- -0.6

b.q12 <- r.xy * y.sd / x.sd
a.q12 <- y.mean - b.q12 * x.mean
```

$b = r_{xy} \frac{\sigma_{y}}{\sigma_{x}} = (`r r.xy`)\frac{`r y.sd`}{`r x.sd`} = `r b.q12`$ 

$a = \bar{y} - b\bar{x} = `r y.mean` - (`r b.q12`)`r x.mean` = `r a.q12`$

Regression line predicting y from x:

$\hat{y} = `r b.q12`x + `r a.q12`$

## Chapter 14, Q14.

True

## Chapter 14, Q16.

False. If r is 0.8, then 64% of the variance is explained.

## Chapter 14, Q18.

### Prepare Environment

Load libraries.

```{r libraries}
library(xlsx)
```

Download data from online Url to subdirectory local to this RStudio project.

```{r download-data}
if (!file.exists("data")) {
  dir.create("data")
}

# !!!This version of file via automated download fails. 
# Remainder of code for assigment assumes manually imported version of .xlsx file located 
# in data subdirectory of working (.rproj) directory
am.url <- "http://onlinestatbook.com/2/case_studies/data/angry_moods.xls"
am.file <- "./data/angry_moods.xls"
if (!file.exists(am.file)) {
  download.file(am.url, destfile = am.file)
}
```

Import data.

```{r import-data}
am.df <- read.xlsx(am.file, 
                   sheetName = "angry_moods.txt", 
                   header = TRUE,
                   colClasses = rep("integer", 7))
```

### Observe scatter plot

```{r q18-plot}
plot(am.df$Control.Out, am.df$Anger.Out)
```

### Find regression line for predicting Anger.Out from Control.Out

```{r q18}
coao.n <- nrow(am.df)
co.mean <- mean(am.df$Control.Out)
ao.mean <- mean(am.df$Anger.Out)
cor.coao <- cor(am.df$Control.Out, am.df$Anger.Out, method = "pearson")

# calculate slope and intercept of simple linear regression  
b.ao <- cor.coao * (sd(am.df$Anger.Out) / sd(am.df$Control.Out))
a.ao <- ao.mean - b.ao * co.mean
coao.predicted <- b.ao * am.df$Control.Out + a.ao
coao.residual <- am.df$Anger.Out - coao.predicted

# collect data points into a data frame
coao.regression <- data.frame(seq(1, coao.n), am.df$Control.Out, am.df$Anger.Out, 
                              coao.predicted, coao.residual, coao.residual^2)
colnames(coao.regression) <- c("id", "Control.Out", "Anger.Out", 
                               "AO.Predicted", "AO.Resid", "AO.SqResid")

# Check result using lm()
coao.lm <- lm(Anger.Out ~ Control.Out, data = coao.regression)
coao.coeffs <- coefficients(coao.lm)
```

$\hat{y}_{AO} = bx_{CO} + a$

### a.

$b = r_{COAO} \frac{\sigma_{AO}}{\sigma_{CO}}$

$r_{COAO} = `r cor.coao`$

$b = r_{COAO} \frac{\sigma_{AO}}{\sigma_{CO}} = (`r cor.coao`)\frac{`r sd(am.df$Anger.Out)`}{`r sd(am.df$Control.Out)`} = `r b.ao`$ 

### b.

$a = \bar{y}_{AO} - b\bar{x}_{CO} = `r ao.mean` - (`r b.ao`)`r co.mean` = `r a.ao`$

Regression line:

$\hat{y}_{AO} = `r b.ao`x_{CO} + `r a.ao`$

### c.

Plot regression, calculated manually:

```{r q18-test}
plot(coao.regression$Control.Out, coao.regression$Anger.Out, col = "blue", main = "Manual Linear Regression")
points(coao.regression$Control.Out, coao.regression$AO.Predicted, col = "red")
```

Plot histogram of residuals.

```{r q18-resid}
hist(coao.regression$AO.Resid)
```

The relationship appears to be approximately linear. The errors of prediction appear to be distributed normally. However, further analysis may be necessary to determine if the distribution is homoscedastic as the variance around the regression line does not appear wholly consistent.

### d.

#### Null hypothesis:

$H_{0}: b = 0$

$H_{A}: b <> 0$

#### Test:

```{r q18-signif}
coao.ssx <- sum((am.df$Control.Out - co.mean)^2)
coao.ssy <- sum((am.df$Anger.Out - ao.mean)^2)
coao.see.r <- sqrt(((1 - cor.coao^2) * coao.ssy) / (coao.n - 2))
coao.see <- sqrt(sum(coao.regression$AO.SqResid) / (coao.n - 2))
coao.sd.b <- coao.see / sqrt(coao.ssx)

coao.t <- b.ao / coao.sd.b

coao.p <- 2 * pt(coao.t, df = coao.n - 2, lower.tail = FALSE)
coao.t.05 <- qt(1 - .05 / 2, df = coao.n - 2)
```

$t = \frac{statistic - \text{hypothesized value}}{\text{standard error of the statistic}}$

Where $b$ is the slope of the regression line predicting Anger.Out from Control.Out, and $s_{b}$ is the standard error of $b$:

$t = \frac{b - 0}{s_{b}}$

Where $s_{est}$ is the standard error of the estimate, SSX is the sum of squared deviations of Control.Out ($x_{CO}$) from $\bar{x}_{CO}$ and SSY is the sum of squared deviations of Anger.Out ($x_{AO}$) from $\bar{y}_{AO}$:

$s_{b} = \frac{s_{est}}{\sqrt{SSX}}$

$SSX = \sum (x_{CO}-\bar{x}_{CO})^2 = `r coao.ssx`$

$s_{est} = \sqrt{\frac{(1-r^2)SSY}{N - 2}}$

$SSY = \sum (y_{AO}-\bar{y}_{AO})^2 = `r coao.ssy`$

$s_{est} = \sqrt{\frac{(1-`r cor.coao`^2)`r coao.ssy`}{`r coao.n` - 2}} = `r coao.see`$

$s_{b} = \frac{s_{est}}{\sqrt{SSX}} = \frac{`r coao.see`}{\sqrt{`r coao.ssx`}} = `r coao.sd.b`$

$t = \frac{b - 0}{s_{b}} = \frac{`r b.ao`}{`r coao.sd.b`} = `r coao.t`$

What is the probability of getting a t as large or larger or as small or smaller in our sampling distribution?

Degrees of Freedom = `r coao.n - 2`

$P(t < -`r coao.t` \; or \; t > `r coao.t`) = `r coao.p`$

Alternately, what are the t critical values for a two-tailed test of alpha = 0.5 and df = `r coao.n - 2`?

$`r -coao.t.05`, `r coao.t.05`$

We reject the null hypothesis; the slope is significantly different from 0 within 95% confidence.

### e.

## Chapter 16, Q1.

Log transformations can be used to make highly skewed distributions less skewed. This can be valuable both for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics.

## Chapter 16, Q2.
10^3
## Chapter 16, Q3.

## Chapter 16, Q4.

## Chapter 16, Q5.
